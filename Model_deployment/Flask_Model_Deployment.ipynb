{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZebDMvUig-Dg",
        "outputId": "65ef856a-a3e8-4f5f-c915-a0beb25ce561"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: flask_ngrok in /usr/local/lib/python3.10/dist-packages (0.0.25)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask_ngrok) (2.2.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask_ngrok) (2.27.1)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok) (2.3.0)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok) (8.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.8->flask_ngrok) (2.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (6.0.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0)\n",
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!pip install flask_ngrok\n",
        "!pip install pyngrok\n",
        "!ngrok authtoken 2PeRoZuDhQBsOnJZ5KAEdQB8jlD_6tg7SwMUAMvzAFYeXqdNZ\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask, request, jsonify, render_template\n",
        "import pickle\n",
        "import gdown\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCb9m6BE1ZCq",
        "outputId": "319f79c8-a44f-41ee-eb93-fcd0e58ec65d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from wordcloud import WordCloud\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "HqKRv-VVfsSA",
        "outputId": "3da40a2b-b3fd-4aaf-8bde-47d1426aa392"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1liwwhm01z8NZw9vXTUBz0gL_t98Rrjgm\n",
            "To: /content/NN_model.pkl\n",
            "100%|██████████| 2.45M/2.45M [00:00<00:00, 137MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=16cOGEvt7L_a0E6S3uoGYA5Kh0_lOZUY3\n",
            "To: /content/title_vectorizer.pkl\n",
            "100%|██████████| 85.7k/85.7k [00:00<00:00, 50.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=14M5Lf716q5UtuojgJY2zvYRqP92gLeGg\n",
            "To: /content/body_vectorizer.pkl\n",
            "100%|██████████| 7.00M/7.00M [00:00<00:00, 250MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1vgn9j9wnZLZjX_giwlvcaFT948LfoZWJ\n",
            "To: /content/index.html\n",
            "100%|██████████| 464/464 [00:00<00:00, 1.60MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1i11XE4mctBXo3vduzaepaRriXr7F7qQs\n",
            "To: /content/result.html\n",
            "100%|██████████| 323/323 [00:00<00:00, 1.00MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1QJbmftsUGKGte7MRV0UM0cYLErzdtGEk\n",
            "To: /content/columnsdf.csv\n",
            "100%|██████████| 678k/678k [00:00<00:00, 113MB/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'columnsdf.csv'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gdown.download(id = '1liwwhm01z8NZw9vXTUBz0gL_t98Rrjgm')\n",
        "gdown.download(id = '16cOGEvt7L_a0E6S3uoGYA5Kh0_lOZUY3')\n",
        "gdown.download(id = '14M5Lf716q5UtuojgJY2zvYRqP92gLeGg')\n",
        "gdown.download(id = '1vgn9j9wnZLZjX_giwlvcaFT948LfoZWJ')\n",
        "gdown.download(id = '1i11XE4mctBXo3vduzaepaRriXr7F7qQs')\n",
        "gdown.download(id = '1QJbmftsUGKGte7MRV0UM0cYLErzdtGEk')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWHvohgu4puI"
      },
      "source": [
        "### lematizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3Iwq7lw4o-k"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_sentence(sentence):\n",
        "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
        "\n",
        "    lemmatized_sentence = []\n",
        "    \n",
        "    for word, tag in nltk_tagged:\n",
        "        if tag is None:\n",
        "\n",
        "            lemmatized_sentence.append(word)\n",
        "        else:\n",
        "          try:\n",
        "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
        "          except:\n",
        "            lemmatized_sentence.append(word)\n",
        "\n",
        "    return \" \".join(lemmatized_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzbaRnpB4vdE"
      },
      "source": [
        "### stopword remover"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsICclsc5S7z"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(rev):\n",
        "\n",
        "    review_tokenized = word_tokenize(rev)\n",
        "    rev_new = \" \".join([i for i in review_tokenized  if i not in stop_words])\n",
        "    return rev_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3AcRmsP5Wtw"
      },
      "source": [
        "# app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_tnvxFkgimP"
      },
      "outputs": [],
      "source": [
        "with open('/content/NN_model.pkl', 'rb') as f:\n",
        "    model = pickle.load(f)\n",
        "\n",
        "with open('/content/title_vectorizer.pkl', 'rb') as f:\n",
        "    title_vectorizer = pickle.load(f)\n",
        "\n",
        "with open('/content/body_vectorizer.pkl', 'rb') as f:\n",
        "    body_vectorizer = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XB2Ui6MmguxH"
      },
      "outputs": [],
      "source": [
        "app = Flask('Real_or_Fake_News', template_folder='/content/')\n",
        "run_with_ngrok(app)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "9n3XdyEa_5gM",
        "outputId": "7db62d74-645f-4142-b2bb-cff1153dc1fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app 'Real_or_Fake_News'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Running on http://ecef-35-190-135-25.ngrok-free.app\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [12/May/2023 06:51:56] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/May/2023 06:51:57] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "<ipython-input-17-9ef5780f9399>:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(new_row, ignore_index=True)\n",
            "<ipython-input-17-9ef5780f9399>:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['Title'] = df['Title'].str.replace(\"[^a-zA-Z0-9]\", \" \")\n",
            "<ipython-input-17-9ef5780f9399>:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['Body'] = df['Body'].str.replace(\"[^a-zA-Z0-9]\", \" \")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 58ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [12/May/2023 06:52:27] \"POST / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/May/2023 06:52:33] \"GET / HTTP/1.1\" 200 -\n",
            "<ipython-input-17-9ef5780f9399>:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(new_row, ignore_index=True)\n",
            "<ipython-input-17-9ef5780f9399>:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['Title'] = df['Title'].str.replace(\"[^a-zA-Z0-9]\", \" \")\n",
            "<ipython-input-17-9ef5780f9399>:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['Body'] = df['Body'].str.replace(\"[^a-zA-Z0-9]\", \" \")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 35ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [12/May/2023 06:52:41] \"POST / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/May/2023 06:52:44] \"GET / HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "@app.route('/', methods=['GET', 'POST'])\n",
        "def index():\n",
        "    if request.method == 'POST':\n",
        "        title = str(request.form['title'])\n",
        "        body = str(request.form['body'])\n",
        "        \n",
        "        df = pd.DataFrame(columns = ['Title','Body'])\n",
        "        new_row= {'Title': title,'Body': body}\n",
        "        df = df.append(new_row, ignore_index=True)\n",
        "\n",
        "        df['Title'] = [str(i).lower() for i in df['Title']]\n",
        "        df['Title'] = df['Title'].str.replace(\"[^a-zA-Z0-9]\", \" \") \n",
        "        df['Body'] = [str(i).lower() for i in df['Body']]\n",
        "        df['Body'] = df['Body'].str.replace(\"[^a-zA-Z0-9]\", \" \") \n",
        "        \n",
        "        df['Title'] = df['Title'].apply(lambda x: lemmatize_sentence(x))\n",
        "        df['Body'] = df['Body'].apply(lambda x: lemmatize_sentence(x))\n",
        "\n",
        "        df['Title'] = [remove_stopwords(r) for r in df['Title']]\n",
        "        df['Body'] = [remove_stopwords(r) for r in df['Body']]\n",
        "\n",
        "        t = title_vectorizer.fit_transform(df['Title'])\n",
        "        feature_names = title_vectorizer.get_feature_names_out()\n",
        "        vecdf = pd.DataFrame(t.toarray(), columns = feature_names)\n",
        "        df = df.merge(vecdf,how = 'outer',left_index=True,right_index=True)        \n",
        "\n",
        "        t = body_vectorizer.fit_transform(df['Body'])\n",
        "        feature_names = body_vectorizer.get_feature_names_out()\n",
        "        vecdf = pd.DataFrame(t.toarray(), columns = feature_names)\n",
        "        df = df.merge(vecdf,how = 'outer',left_index=True,right_index=True)\n",
        "        \n",
        "        del df['Title']\n",
        "        del df['Body']\n",
        "\n",
        "        colsdf = pd.read_csv('/content/columnsdf.csv')\n",
        "        colsdf.loc[0] = 0\n",
        "        \n",
        "        merged_df = colsdf.copy()\n",
        "        for column in colsdf.columns:\n",
        "            if column in df.columns:\n",
        "                merged_df[column] = df[column]\n",
        "        merged_df\n",
        "\n",
        "        prediction = model.predict(merged_df.values)\n",
        "        a = a = ( 1 if prediction[0][0] >= 0.5 else 0)\n",
        "        \n",
        "        return render_template('result.html', prediction=a)\n",
        "    return render_template('index.html')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}