# -*- coding: utf-8 -*-
"""Flask_Model_Deployment

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1siulFozPxBfW2hcPRgF6T4TI__Az0aWf
"""

# !pip install flask_ngrok
# !pip install pyngrok
from flask import Flask, request, jsonify, render_template
import pickle
import gdown 
import re
import os

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
from wordcloud import WordCloud
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
from nltk.corpus import stopwords
from nltk import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
nltk.download('stopwords')
stop_words = stopwords.words('english')
nltk.download('omw-1.4')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

os.makedirs('content', exist_ok=True)
os.chdir('content')

import platform

# Get the OS version
os_version = platform.platform()

# Print the OS version
print("OS Version:", os_version)

gdown.download(id = '1liwwhm01z8NZw9vXTUBz0gL_t98Rrjgm')
gdown.download(id = '16cOGEvt7L_a0E6S3uoGYA5Kh0_lOZUY3')
gdown.download(id = '14M5Lf716q5UtuojgJY2zvYRqP92gLeGg')
gdown.download(id = '1vgn9j9wnZLZjX_giwlvcaFT948LfoZWJ')
gdown.download(id = '1i11XE4mctBXo3vduzaepaRriXr7F7qQs')
gdown.download(id = '1QJbmftsUGKGte7MRV0UM0cYLErzdtGEk')

"""### lematizer"""

lemmatizer = WordNetLemmatizer()

def lemmatize_sentence(sentence):
    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  

    lemmatized_sentence = []
    
    for word, tag in nltk_tagged:
        if tag is None:

            lemmatized_sentence.append(word)
        else:
          try:
            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))
          except:
            lemmatized_sentence.append(word)

    return " ".join(lemmatized_sentence)

"""### stopword remover"""

def remove_stopwords(rev):

    review_tokenized = word_tokenize(rev)
    rev_new = " ".join([i for i in review_tokenized  if i not in stop_words])
    return rev_new

"""# app"""

with open('/app/real-fake-news-neural-network/content/NN_model.pkl', 'rb') as f:
    model = pickle.load(f)

with open('/app/real-fake-news-neural-network/content/title_vectorizer.pkl', 'rb') as f:
    title_vectorizer = pickle.load(f)

with open('/app/real-fake-news-neural-network/content/body_vectorizer.pkl', 'rb') as f:
    body_vectorizer = pickle.load(f)

app = Flask('Real_or_Fake_News', template_folder='/app/real-fake-news-neural-network/content/')

# !pip install pipreqs
# !pip freeze > requirements.txt

@app.route('/', methods=['GET', 'POST'])
def index():
    if request.method == 'POST':
        title = str(request.form['title'])
        body = str(request.form['body'])
        
        df = pd.DataFrame(columns = ['Title','Body'])
        new_row= {'Title': title,'Body': body}
        df = df.append(new_row, ignore_index=True)

        df['Title'] = [str(i).lower() for i in df['Title']]
        df['Title'] = df['Title'].str.replace("[^a-zA-Z0-9]", " ") 
        df['Body'] = [str(i).lower() for i in df['Body']]
        df['Body'] = df['Body'].str.replace("[^a-zA-Z0-9]", " ") 
        
        df['Title'] = df['Title'].apply(lambda x: lemmatize_sentence(x))
        df['Body'] = df['Body'].apply(lambda x: lemmatize_sentence(x))

        df['Title'] = [remove_stopwords(r) for r in df['Title']]
        df['Body'] = [remove_stopwords(r) for r in df['Body']]

        t = title_vectorizer.fit_transform(df['Title'])
        feature_names = title_vectorizer.get_feature_names_out()
        vecdf = pd.DataFrame(t.toarray(), columns = feature_names)
        df = df.merge(vecdf,how = 'outer',left_index=True,right_index=True)        

        t = body_vectorizer.fit_transform(df['Body'])
        feature_names = body_vectorizer.get_feature_names_out()
        vecdf = pd.DataFrame(t.toarray(), columns = feature_names)
        df = df.merge(vecdf,how = 'outer',left_index=True,right_index=True)
        
        del df['Title']
        del df['Body']

        colsdf = pd.read_csv('/app/real-fake-news-neural-network/content/columnsdf.csv')
        colsdf.loc[0] = 0
        
        merged_df = colsdf.copy()
        for column in colsdf.columns:
            if column in df.columns:
                merged_df[column] = df[column]
        merged_df

        prediction = model.predict(merged_df.values)
        a = ( 1 if prediction[0][0] > 0.5 else 0)
        
        return render_template('result.html', prediction=a)
    return render_template('index.html')

if __name__ == '__main__':
    app.run()